{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Ngram Language Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toWords(text):  # separates punctuation\n",
    "    token = re.compile(u'[^\\s\\w-]|[\\w-]+', re.UNICODE) #pattern to identify words, punct, and hyphenated words\n",
    "    tokens = re.findall(token, text)\n",
    "    return [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Ngrams\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "def getNGrams(text, n):\n",
    "  text = (\" \" * (n - 1)) + text + \" \"\n",
    "  return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "\n",
    "\"\"\" Creates the conditional frequency distribution\n",
    "\n",
    "    @param words a list of words from the training data\n",
    "    @param n the length of an n-gram\n",
    "    @return a mapping of context (unique substring of the first n-1 characters)\n",
    "    to endings (last character) and their frequencies\n",
    "\n",
    "  for each sentence in list of sentences:\n",
    "        get n-grams using above method\n",
    "        group by first n-1 characters (context)\n",
    "        for each unique context, create a map of last character counts\n",
    "          e.g., if you have the ngrams \"chair\" and \"chain\":\n",
    "                the context would be \"chai\" and the last character counts\n",
    "                would be \"r -> 1\" and \"n -> 1\"\n",
    "        add the [ context -> last character counts ] mapping to your\n",
    "        ultimate CFD (also a map)\n",
    "          e.g., your CFD  would have lots of entries that look like:\n",
    "                \"chai\" -> {\"r\" -> 1, \"n -> 1}\n",
    "                as an key-value pair.\n",
    "\"\"\"\n",
    "def getConditionalCounts(words, n):\n",
    "  condCounts = {}\n",
    "  for word in words:\n",
    "    ngrams = getNGrams(word, n)\n",
    "    for gram in ngrams:\n",
    "      context, lastChar = gram[:n - 1], gram[-1]\n",
    "      condCounts.setdefault(context, {}).setdefault(lastChar, 0)\n",
    "      condCounts[context][lastChar] += 1\n",
    "  return condCounts\n",
    "\n",
    "\n",
    "\n",
    "class CharNGram:\n",
    "  def __init__(self, language, conditionalCounts, n, numLetters=26):\n",
    "    self.language = language\n",
    "    self.condCounts = conditionalCounts\n",
    "    self.n = n\n",
    "    self.numLetters = numLetters\n",
    "    self._getNormalizedCounts()\n",
    "\n",
    "  def _getNormalizedCounts(self):\n",
    "    for ctx, counts in self.condCounts.items():\n",
    "      for lastChar, count in counts.items():\n",
    "        self.condCounts[ctx][lastChar] = (count + 1)/float(self.numLetters)\n",
    "\n",
    "    \"\"\"\n",
    "    Using conditional frequency distribution,\n",
    "    calculate and return p(c | ctx)\n",
    "    \"\"\"\n",
    "  def ngramProb(self, ctx, c):\n",
    "    return self.condCounts.get(ctx, {}).get(c, 1.0/float(self.numLetters))\n",
    "\n",
    "    \"\"\" Multiply ngram probabilites for each ngram in word \"\"\"\n",
    "  def wordProb(self, word):\n",
    "    prob = 1.0\n",
    "    for ctx, counts in getConditionalCounts([word], self.n).items():\n",
    "      for lastChar, count in counts.items():\n",
    "        prob *= self.ngramProb(ctx, lastChar) * count\n",
    "    return math.log(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSwitchedLanguageModel:\n",
    "  def __init__(self, models):\n",
    "    self.models = models\n",
    "\n",
    "  def guess(self, word):\n",
    "    highestProb = max(model.wordProb(word.lower()) for model in self.models)\n",
    "    guess = [model for model in self.models\n",
    "                   if model.wordProb(word.lower()) == highestProb]\n",
    "    return guess[0].language\n",
    "\n",
    "  def prob(self, language, word):\n",
    "    return [model for model in self.models\n",
    "                  if model.language == language][0].wordProb(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation:\n",
    "    def __init__(self, words, cslm):\n",
    "        self.words = words\n",
    "        self.cslm = cslm\n",
    "        self.tagSet = [u\"Eng\", u\"Spn\"]\n",
    "        self.lang = []\n",
    "        self.engProbs = []\n",
    "        self.spnProbs = []\n",
    "        self._generateTags()\n",
    "\n",
    "    def _generateTags(self):\n",
    "        punct_pattern = re.compile(u'[^\\w\\s]', re.UNICODE)\n",
    "        for k, word in enumerate(self.words):\n",
    "\n",
    "            # annotate punct and move to next token\n",
    "            if re.match(punct_pattern, word):\n",
    "                self.lang.append('Punct')\n",
    "                self.engProbs.append(\"NA\")\n",
    "                self.spnProbs.append(\"NA\")\n",
    "                continue\n",
    "\n",
    "            # annotate numbers and move to next token\n",
    "            if re.match(u'\\d', word):\n",
    "                self.lang.append('Num')\n",
    "                self.engProbs.append(\"NA\")\n",
    "                self.spnProbs.append(\"NA\")\n",
    "                continue\n",
    "\n",
    "            # for lexical tokens determine lang tag\n",
    "            spnProb = self.cslm.prob(\"Spn\", word); self.spnProbs.append(spnProb)\n",
    "            engProb = self.cslm.prob(\"Eng\", word); self.engProbs.append(engProb)\n",
    "            \n",
    "            if .9 < engProb/spnProb < 1.1:\n",
    "                print(word)\n",
    "                try:\n",
    "                    lang = self.lang[k-1] # default to previous language tag\n",
    "                    self.lang.append(lang + \"?\")\n",
    "                except IndexError:\n",
    "                    lang = self.cslm.guess(word)\n",
    "                    self.lang.append(lang)\n",
    "            else:\n",
    "                lang = self.cslm.guess(word)\n",
    "                self.lang.append(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class langDetector:\n",
    "    def __init__(self):\n",
    "        self.cslm = self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        n = 4\n",
    "        engPath = \"Data/EnglishTrainingCorpus.txt\"\n",
    "        engData = toWords(io.open(engPath, 'r', encoding='utf8').read())\n",
    "\n",
    "        spnPath = \"Data/SpanishTrainingCorpus.txt\"\n",
    "        spnData = toWords(io.open(spnPath, 'r', encoding='utf8').read())\n",
    "\n",
    "        enModel = CharNGram('Eng', getConditionalCounts(engData, n), n)\n",
    "        esModel = CharNGram('Spn', getConditionalCounts(spnData, n), n)\n",
    "\n",
    "        return CodeSwitchedLanguageModel([enModel, esModel])\n",
    "\n",
    "    def tag(self, text_list):\n",
    "        # annotation_lists = []\n",
    "        hmm = Annotation(text_list, self.cslm)\n",
    "        return hmm.lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langDetector = langDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spn', 'Eng']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langDetector.tag([\"hola\", \"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KC = \"\"\"En Sudáfrica, llegué a creer que la vida era eso, precisely: the otherwordly, ephemeral beauty de los jacarandaes and, in equal measure, the clawing loneliness of having just three people en todo el continente africano.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "africano\n",
      "42 42\n",
      "en Spn\n",
      "sudáfrica Spn\n",
      ", Punct\n",
      "llegué Spn\n",
      "a Spn?\n",
      "creer Spn\n",
      "que Spn\n",
      "la Spn\n",
      "vida Spn\n",
      "era Spn\n",
      "eso Spn\n",
      ", Punct\n",
      "precisely Eng\n",
      ": Punct\n",
      "the Eng\n",
      "otherwordly Eng\n",
      ", Punct\n",
      "ephemeral Eng\n",
      "beauty Eng\n",
      "de Spn\n",
      "los Spn\n",
      "jacarandaes Spn\n",
      "and Eng\n",
      ", Punct\n",
      "in Eng\n",
      "equal Eng\n",
      "measure Eng\n",
      ", Punct\n",
      "the Eng\n",
      "clawing Eng\n",
      "loneliness Eng\n",
      "of Eng\n",
      "having Eng\n",
      "just Eng\n",
      "three Eng\n",
      "people Eng\n",
      "en Spn\n",
      "todo Spn\n",
      "el Spn\n",
      "continente Spn\n",
      "africano Spn?\n",
      ". Punct\n"
     ]
    }
   ],
   "source": [
    "KC_tokens = toWords(KC)\n",
    "KC_tags = langDetector.tag(KC_tokens)\n",
    "print(len(KC_tokens), len(KC_tags))\n",
    "for token, tag in zip(KC_tokens, KC_tags):\n",
    "    print(token,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn / ¡Te toca!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the character ngram model on codeswitched book and interview. \n",
    "For each document, what percentage is Spanish? English? Which is more balanced between the two languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_book = open(\"Data/CodeswitchedBook.txt\").read()\n",
    "cs_interview = open(\"Data/Spanish_in_Texas_subset.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
